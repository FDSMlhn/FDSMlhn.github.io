<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="hexo," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.2" />






<meta name="description" content="During the learning of RNN, I undergone a very tough time to deduce the BPTT for vanilla RNN. There are some pieces of work online that try to explain this process(click here or here). However, I fou">
<meta property="og:type" content="article">
<meta property="og:title" content="Derivation of Back Propagation Through Time (BPTT) - Vanilla Recurrent Neural Network(RNN)">
<meta property="og:url" content="http://yoursite.com/2017/10/22/Derivation of Back Propagation Through Time (BPTT) - Vanilla Recurrent Neural Network(RNN)/index.html">
<meta property="og:site_name" content="Nevermore">
<meta property="og:description" content="During the learning of RNN, I undergone a very tough time to deduce the BPTT for vanilla RNN. There are some pieces of work online that try to explain this process(click here or here). However, I fou">
<meta property="og:updated_time" content="2017-11-02T20:00:33.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Derivation of Back Propagation Through Time (BPTT) - Vanilla Recurrent Neural Network(RNN)">
<meta name="twitter:description" content="During the learning of RNN, I undergone a very tough time to deduce the BPTT for vanilla RNN. There are some pieces of work online that try to explain this process(click here or here). However, I fou">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    }
  };
</script>




  <link rel="canonical" href="http://yoursite.com/2017/10/22/Derivation of Back Propagation Through Time (BPTT) - Vanilla Recurrent Neural Network(RNN)/"/>


  <title> Derivation of Back Propagation Through Time (BPTT) - Vanilla Recurrent Neural Network(RNN) | Nevermore </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="//schema.org/WebPage" lang="en">

  










  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="//schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Nevermore</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Just begins.</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule" rel="section">
            
            Schedule
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="//schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Derivation of Back Propagation Through Time (BPTT) - Vanilla Recurrent Neural Network(RNN)
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2017-10-22T00:00:00-07:00" content="2017-10-22">
              2017-10-22
            </time>
          </span>

          

          
            
          

          

          
          

          
              &nbsp; | &nbsp;
              <span class="page-pv"><i class="fa fa-file-o"></i>Views
              <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
              </span>
          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <hr>
<p>During the learning of RNN, I undergone a very tough time to deduce the BPTT for vanilla RNN. There are some pieces of work online that try to explain this process(click <a href="https://github.com/go2carter/nn-learn/blob/master/grad-deriv-tex/rnn-grad-deriv.pdf" target="_blank" rel="external">here</a> or <a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/" target="_blank" rel="external">here</a>). However, I found the derivation relevant to hidden state unsatisfying. Specificly, They are mixing up the partial derivative and total derivative, which is a total disaster for beginners to understand it. So I want to write this blog to put down some tough points in the process. And meanwhile, I will also skip the parts that’s been explained thoroughly in the posts above. Hopefully, this blog will help you save a bunch of time to wrap you head around this fantastic algorithm.<br><a id="more"></a><br>The blog is generally for ones who have already known at least the setting of Neural Network and RNN. The body of this blog is going to be constructed as following:</p>
<ol>
<li><strong>Math Recap: the difference between partial derivative and total derivative.</strong></li>
<li><strong>Paritial derivative for Neural Network</strong></li>
<li><strong>The essence of BPTT</strong></li>
</ol>
<hr>
<h2 id="1-Math-Recap-the-difference-between-partial-derivative-and-total-derivative"><a href="#1-Math-Recap-the-difference-between-partial-derivative-and-total-derivative" class="headerlink" title="1. Math Recap: the difference between partial derivative and total derivative."></a>1. Math Recap: the difference between partial derivative and total derivative.</h2><p>I see a lot of people misusing these two terms and in most cases, it won’t cause any trouble. But for involved BPTT, I think it will help you understand it if we clarify it beforehand. </p>
<p>In term of math symbols, partial derivative looks like $\frac{\partial{y}}{\partial{y}}$ and total derivative looks like $\frac{\mathrm{d}y}{\mathrm{d}x}$. The important discrepancy between them is that when we are doing partial derivative, we assume all other parts are static. They won’t change even if it won’t be the case in the real world. For example, assume $F(x)= g(x)f(x)$, $g(x)$ and $f(x)$ are both the function of $x$ and generally will change when the other one changes. If we take the total derivative of $F(x)$ to $x$, we’ve all seen formula like this: </p>
<p>$$<br>\begin{aligned}<br>\frac{\mathrm{d}F(x)}{\mathrm{d}x} &amp;= \frac{\partial{F(x)}}{\partial{g(x)}} \frac{\mathrm{d}g(x)}{\mathrm{d}x} + \frac{\partial{F(x)}}{\partial{f(x)}} \frac{\mathrm{d}f(x)}{\mathrm{d}x} \\<br>\frac{\partial{F(x)}}{\partial{g(x)}} &amp;= f(x) \\<br>\frac{\partial{F(x)}}{\partial{f(x)}} &amp;= g(x)<br>\end{aligned}<br>$$ </p>
<p>The partial derivative neglects the change of g(x) while changing f(x). If we want $\frac{\mathrm{d}F(x)}{\mathrm{d}g(x)}$, it then will depend on the relationship between function $g$ and $f$. </p>
<hr>
<h2 id="2-Partial-derivative-for-Neural-Network"><a href="#2-Partial-derivative-for-Neural-Network" class="headerlink" title="2. Partial derivative for Neural Network"></a>2. Partial derivative for Neural Network</h2><p>In this part, I want to talk about the backpropagation in vanilla neural networks to enhance your understanding of partial derivatives and more importantly, to elaborate some tricks to use when we are doing the back propagation. If you already have a fully comprehension towards BP in Neural Network, feel free to skip this section.</p>
<p>Without the loss of generality, I will use two-layer neural network as our example(one hidden layer), the model is for a multilabel classification problem:<br>$$<br>\begin{aligned}<br>\mathbf{z}^{(1)} &amp;= \mathbf{X}\mathbf{W_1} + \mathbf{b_1} \\<br>\mathbf{a}^{(1)} &amp;= f^{(1)}(\mathbf{z}^{(1)}) \\<br>\mathbf{z}^{(2)} &amp;= \mathbf{a}^{(1)}\mathbf{W_2} + \mathbf{b_2} \\<br>\hat{\mathbf{Y}} &amp;= g(\mathbf{z}^{(2)}) \\<br>Loss &amp;= \sum^{N}_{i=1} L(y_i, \hat{\mathbf{Y_i}})<br>\end{aligned}<br>$$</p>
<p>The dimensions of $\mathbf{X}$, $\mathbf{W_1}$, $\mathbf{z}^{(1)}$,$\mathbf{W_2}$, $\hat{\mathbf{Y}}$ will be $(N, D)$, $(D, D_1)$, $(N, D_1)$, $(D_1, D_2)$, $(N, C)$. The superscript 1 and 2 indicates the layer, subscript i indicate the corresponding row of data.</p>
<p>When we are trying to calculate the total derivative to the $W_1$, there will be generally two ways to do this. One formal way and one tricky way:</p>
<h3 id="Formal-derivation"><a href="#Formal-derivation" class="headerlink" title="Formal derivation:"></a>Formal derivation:</h3><p>The process is straightforward, we only need to write down it as the chain rule suggests.<br>$$<br>\frac{\mathrm{d}L}{\mathrm{d}{\mathbf{W_1}}} =<br>\frac{\mathrm{d}{\mathbf{Z}^{(1)}}}{\mathrm{d}{\mathbf{W_1}}}<br>\frac{\partial{\mathbf{Z}^{(2)}}}{\partial{\mathbf{Z}^{(1)}}}<br>\frac{\partial{\mathbf{Y}}}{\partial{\mathbf{Z}^{(2)}}}<br>\frac{\partial{L}}{\partial{\mathbf{Y}}}<br>$$</p>
<p>Although the product of all these things is a simple matrix. However, there are a lot of waste in the process. We have to see that the intermediate matrice are some 4 dimemsional tensors, e.g. $\frac{\mathrm{d}{\mathbf{Z}^{(1)}}}{\mathrm{d}{\mathbf{W_1}}}$ is a tensor of $(D, D_1, N, D_1)$ dimension. For every element $W_{1ij}$ among the $(D, D_1)$ elements in $W_1$. There exists a $(N, D_1)$ matrix of derivative of $\mathbf{Z}^{(1)}$ to $W_{1ij}$. And among elements $N * D_1$ of this matrix, only N elements are none zero(the ones of neuron j in layer 1, namely the column j of matrix $\mathbf{Z}^{(1)}$). So as a result, a prevailing tricky one comes into our views.</p>
<h3 id="Tricky-derivation"><a href="#Tricky-derivation" class="headerlink" title="Tricky derivation:"></a>Tricky derivation:</h3><p>Instead of computing the result all in the end, we will try to decrease this problem and conquer it. For example, in the process, we will try to derive $\frac{\partial{L}}{\partial{\mathbf{Z}^{(2)}}}$ first and go on. In the Python realization, we will note this as a $\mathrm{d}\mathbf{Z}^{(2)}$, which loos like a total derivative sort of thing although it’s a partial derivative. Thus, the <strong>partial</strong> derivative will be like waterflow and easier to compute. On the stage of this, we will furthur consider every element of next matrix to which we shall take derivative, and try to summarize them into a compact matrix multiplication. Like the example we discuss in the previous method, we will try to derive $\frac{\mathrm{d}L}{\mathrm{d}{\mathbf{W_1}}}$ with $\mathrm{d}\mathbf{Z}^{(2)}$. Then, we can just see what we get if we want to have $\frac{\mathrm{d}L}{\mathrm{d}{\mathbf{W_{1ij}}}}$. With ease we know that it is the inner product of $\mathrm{d}\mathbf{Z}^{(2)}_{j}$ and $\mathbf{X}_{i}$, which are both shape of $(N,)$. As a result, the 4 dimensional tensor multiplication in the above will become:</p>
<p>$$<br>\frac{\mathrm{d}L}{\mathrm{d}{\mathbf{W_1}}} = \mathrm{X}^{T} \mathrm{d}\mathbf{Z}^{(2)}<br>$$</p>
<p>And of course, this tricky derivation is widely used in the algorithm for BP for NN.</p>
<hr>
<h2 id="3-The-essence-of-BPTT-in-vanilla-RNN"><a href="#3-The-essence-of-BPTT-in-vanilla-RNN" class="headerlink" title="3. The essence of BPTT in vanilla RNN"></a>3. The essence of BPTT in vanilla RNN</h2><p>After previous discussion, we will try to deal with this hardcore problem!</p>
<p>First, we will describe the model and it’s parameters:</p>
<p>$$<br>\begin{aligned}<br>\mathbf{h}^{(t)} &amp;=\tanh\left( \mathbf{b} + \mathbf{h}^{(t-1)} \mathbf{W_h} + \mathbf{x}^{(t)} \mathbf{W_x} \right) \\<br>\mathbf{o}^{(t)} &amp;= \mathbf{c} +\mathbf{h}^{(t)} \mathbf{V} \\<br>\mathbf{\hat{y}}^{(t)} &amp;= softmax\left( \mathbf{o}^{(t)} \right) \\<br>\mathbf{L}^{(t)} &amp;= L(\mathbf{y}^{(t)}, \mathbf{\hat{y}}^{(t)}) \\<br>L &amp;= \sum^{T}_{t=1} \sum^{N}_{i=1} L^{(t)}_{i}<br>\end{aligned}<br>$$</p>
<p>Here, we also want to take input as a minibatch of N entries. The dimensions of $\mathbf{x}^{(t)}$, $\mathbf{h}^{(t-1)}$, $\mathbf{W_h}$, $\mathbf{W_x}$, $\mathbf{V}$, $\mathbf{L}^{(t)}$ are $(N,D)$, $(N,H)$, $(H,H)$, $(D,H)$, $(D, C)$,$(N,)$ respectively. $L^{(t)}_{i}$ is the $i$-th element of $\mathbf{L}^{(t)}$. The Loss function is the common cross-entropy loss.</p>
<p>The most headache problem involve the derivative to $\mathbf{W_h}$, $\mathbf{W_x}$ and $\mathbf{b}$. Because they are in all $\mathbf{h}^{(t)}$ so the derivatives get some sort of recursive fashion. It’s quite a thorny thing to deal with. So here, we will cut directly to this part. I will put down the derivation of the derivative of $L$ to $\mathbf{W_h}$. Others will leave as exercises for you:)</p>
<p>Goal: Find the total derivative $ \frac{\mathrm{d}L}{\mathrm{d}\mathbf{W_h}}$<br>$$<br>\frac{\mathrm{d}L}{\mathrm{d}\mathbf{W_h}}= \sum^{T}_{t=1} \frac{\mathrm{d}L^{(t)}}{\mathrm{d}\mathbf{W_h}}<br>$$<br>Notice the $L^{(t)}$ is a scalar and it’s $\sum^{N}_{i=1} L^{(t)}_{i}$. We will just look at one of them to have a sense of how it will end up.</p>
<p>$$<br>\frac{\mathrm{d}L^{(t)}}{\mathrm{d}\mathbf{W_h}} =<br>\frac{\mathrm{d}\mathbf{h}^{(t)}}{\mathrm{d}\mathbf{W_h}}<br>\frac{\partial \mathbf{o}^{(t)}}{\partial \mathbf{h}^{(t)}}<br>\frac{\partial \mathbf{\hat{y}}^{(t)}}{\partial \mathbf{o}^{(t)}}<br>\frac{\partial L^{(t)}}{\partial \mathbf{\hat{y}}^{(t)}}<br>$$</p>
<p>In order to use tricks like BP in the Neural network. We will compute the partial derivative in sequence. Note again the symbol $d$ is just for notation convenience and has nothing to do with <strong>total</strong> derivative.</p>
<p>$$<br>\begin{aligned}<br>d\mathbf{o}^{(t)} &amp;= \frac{\partial L^{(t)}}{\partial \mathbf{o}^{(t)}} = \mathbf{\hat{y}}^{(t)} - \mathbf{1_{y^{(t)}}} \\<br>d\mathbf{h}^{(t)} &amp;=  d\mathbf{o}^{(t)} V^{T}<br>\end{aligned}<br>$$</p>
<p>$\mathbf{1_{y^{(t)}}}$ is a special matrix of same size as $\mathbf{\hat{y}}^{(t)}$. For row $i$ in $\mathbf{1_{y^{(t)}}}$, the $y^{(t)}_{i}$-th element is 1, others are zero. </p>
<p>Then, we will define a intermediate matrix $\mathbf{a}^{(t)}$ to avoid the use of 4 dimensional tensor. Notice the square of $d\mathbf{h}^{(t)}$ is a element-wise square operation.</p>
<p>$$<br>\begin{aligned}<br>\mathbf{a}^{(t)} &amp;=\mathbf{b} + \mathbf{h}^{(t-1)} \mathbf{W_h} + \mathbf{x}^{(t)} \mathbf{W_h} \\<br>\mathbf{h}^{(t)} &amp;=\tanh\left( \mathbf{a}^{(t)} \right) \\<br>d\mathbf{h}^{(t)} &amp;=  d\mathbf{o}^{(t)} V^{T} \\<br>d\mathbf{a}^{(t)} &amp;= 1 - (d\mathbf{h}^{(t)})^2 \\<br>\frac{\mathrm{d}L^{(t)}}{\mathrm{d}\mathbf{W_h}} &amp;=<br>\frac{\mathrm{d}\mathbf{a}^{(t)}}{\mathrm{d}\mathbf{W_h}}<br>d\mathbf{a}^{(t)} \\<br>&amp;=<br>(\frac{\mathrm{d}\mathbf{W_h}}{\mathrm{d}\mathbf{W_h}}<br>\frac{\partial \mathbf{a}^{(t)}}{\partial \mathbf{W_h}} +<br>\frac{\mathrm{d}\mathbf{h}^{(t-1)}}{\mathrm{d}\mathbf{W_h}}<br>\frac{\partial \mathbf{a}^{(t)}}{\partial \mathbf{h}^{(t-1)}} )d\mathbf{a}^{(t)}<br>\end{aligned}<br>$$</p>
<p>For the last equation, the first part is simply. If we use the result from above. The product will simply be $ d{\mathbf{W_h}^{(t)}} =  (\mathbf{h}^{(t-1)})^T d\mathbf{a}^{(t)}$. And then we can furthur integrate the last part of the equation. At last we get:</p>
<p>$$<br>\begin{aligned}<br>\frac{\mathrm{d}L^{(t)}}{\mathrm{d}\mathbf{W_h}} &amp;= d{\mathbf{W_h}^{(t)}} +<br>\frac{\mathrm{d}\mathbf{h}^{(t-1)}}{\mathrm{d}\mathbf{W_h}}<br>d\mathbf{h}^{(t-1)} \\<br>d\mathbf{h}^{(t-1)} &amp;=  d\mathbf{a}^{(t)} (\mathbf{h}^{(t-1)})^T<br>\end{aligned}<br>$$</p>
<p>And then, we find this can be done recursively afterwards. $d\mathbf{h}^{(t-1)}$ will be passed down just like the waterflow in the BP in Neural network. At last, after rearrangement, we will have a final version like this. In order to calculate $d{\mathbf{W_h}^{(1)}}$, there should be a $\mathbf{h}^{(0)}$ as initialization value for the hidden state here.</p>
<p>$$<br>\begin{aligned}<br>\frac{\mathrm{d}L^{(t)}}{\mathrm{d}\mathbf{W_x}} &amp;=<br>\sum^{t}_{i=1}d{\mathbf{W_h}^{(i)}}<br>\end{aligned}<br>$$</p>
<p>In the application, when we are trying to calculate the $\frac{\mathrm{d}L^{(t)}}{\mathrm{d}\mathbf{W_x}}$, for a given timestep $t=t_0 \in \left[ 1, t \right) $, we may first receive the derivative message from $t_0+1$ and then we need to pass down the derivative message. The whole process will looks like this.</p>
<p>$$<br>\begin{aligned}<br>d\mathbf{a}^{(i)} &amp;= 1 - (d\mathbf{h}^{(i)})^2 \\<br>d{\mathbf{W_h}^{(i)}} &amp;=  (\mathbf{h}^{(t-1)})^T d\mathbf{a}^{(t)} \\<br>d{\mathbf{W_x}^{(i)}} &amp;= \ \dots \\<br>d{\mathbf{X}^{(i)}} &amp;= \ \dots \\<br>d{\mathbf{b}^{(i)}} &amp;= \ \dots \\<br>d\mathbf{h}^{(i-1)} &amp;=  d\mathbf{a}^{(i)} (\mathbf{h}^{(i-1)})^T<br>\end{aligned}<br>$$</p>
<p>And then there will be $T$ timestamps we need to compute corresponding to $T$ different $L^{(t)}$. </p>
<hr>
<p>If you find any mistakes in this passage, feel free to contact me through my email: FDSM_lhn@yahoo.com. </p>
<p>You’re welcome to share my passage to other websites or bloggers. <strong>But please add my name and link to this post alongside</strong>. Thank you!</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/hexo/" rel="tag"><i class="fa fa-tag"></i>hexo</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/11/02/Understanding im2col implementation in Python(numpy fancy indexing)/" rel="prev" title="Understanding im2col implementation in Python(numpy fancy indexing)">
                Understanding im2col implementation in Python(numpy fancy indexing) <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="//schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Haonan Li" />
          <p class="site-author-name" itemprop="name">Haonan Li</p>
          <p class="site-description motion-element" itemprop="description">Strong Mind = Success</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">2</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Math-Recap-the-difference-between-partial-derivative-and-total-derivative"><span class="nav-number">1.</span> <span class="nav-text">1. Math Recap: the difference between partial derivative and total derivative.</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Partial-derivative-for-Neural-Network"><span class="nav-number">2.</span> <span class="nav-text">2. Partial derivative for Neural Network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Formal-derivation"><span class="nav-number">2.1.</span> <span class="nav-text">Formal derivation:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tricky-derivation"><span class="nav-number">2.2.</span> <span class="nav-text">Tricky derivation:</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-The-essence-of-BPTT-in-vanilla-RNN"><span class="nav-number">3.</span> <span class="nav-text">3. The essence of BPTT in vanilla RNN</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Haonan Li</span>
</div>

<!-- <div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>
-->

        

<div class="busuanzi-count">

  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv"><i class="fa fa-user"></i># of visitors<span class="busuanzi-value" id="busuanzi_value_site_uv"></span></span>
  

  
    <span class="site-pv"><i class="fa fa-eye"></i>Overall Views<span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span>
  
  
</div>



        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>
  <a href="https://github.com/FDSMlhn"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://camo.githubusercontent.com/567c3a48d796e2fc06ea80409cc9dd82bf714434/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f6c6566745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_left_darkblue_121621.png"></a>
  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.2"></script>



  



  




  
  

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


  

  

  


</body>
</html>
